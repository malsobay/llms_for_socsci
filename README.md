# LLMs for Social Science (WICSS 2024) 
This repository contains data, code, and important links to the "LLMs for Social Science" project at WICSS NYU AD 2024. The main research question the group is trying to answer is: **"When trying to describe a domain using a quantitative representation, how do the representations generated by experts compare to those generated by LLM-augmented efforts, in terms of their contribution to out-of-sample predictive power?"**

## Important links: 
* [Research questions and discussions](https://docs.google.com/document/d/1pi1N0XU2OOrnSNyhnVbKIvFR2TZXqL-40n22JJtiXvU/edit)
* [Lit review](https://docs.google.com/document/d/1EI9u99ZD6ZWUDOaJ65yOdf2lOxrjojDS8uTVvTvitNI/edit)
* [Slides from prompt exercise on "pairwise LLM comparisons to map tasks"](https://docs.google.com/presentation/d/1ZLoBPPAcqTd6QtL1cOlKCvqflxQnHFu7WSUCNHB4LnY/edit?usp=sharing)

## Data files: 
* `24_dimensions_clean.csv`: A description of each of the dimensions from Hu et al, along with source (available from the OSF repo for the paper)
* `102_tasks_with_sources_clean.csv`: Descriptions of the 102 tasks incl stimulus, objective, and source, as collected by Hu et al.
* `task_map.csv`: The mapped values of the 102 tasks along the 24 dimensions, as reported by Hu et al. 
* `gpt_skill_ratings.pkl`: a pickled pandas df with ratings of 99 skills in order of their perceived importance to 100 tasks, as given by GPT-4. 

## Code: 
* `openai_api_demo.ipynb`: Demo notebook showing how to structure and parse OpenAI API calls.
* `llm_task_teams.ipynb`: Notebook containing the API calls and analyses used in last week's prompt exercise. Maybe useful to look through, but please don't run the pairwise comparisons on my API key ðŸ˜…
